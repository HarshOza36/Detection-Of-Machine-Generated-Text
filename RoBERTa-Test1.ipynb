{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torchvision\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nfrom tqdm.notebook import tqdm_notebook\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torchvision.transforms as transforms\nimport random\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pathlib import Path\nfrom PIL import Image\nimport cv2\nimport time\nfrom tqdm.notebook import tqdm_notebook\nimport time\nimport os, shutil\nimport matplotlib.animation as animation\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport sys, time\nfrom nibabel.testing import data_path\nimport nibabel as nib\nfrom PIL import Image\nimport scipy.ndimage as ndi\nimport itertools\nimport time\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch.utils.data import DataLoader\nfrom transformers import DataCollatorWithPadding\nimport tqdm\nimport nltk\nnltk.download('punkt')\nfrom sklearn.metrics import roc_auc_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-26T03:11:05.533850Z","iopub.execute_input":"2023-04-26T03:11:05.534609Z","iopub.status.idle":"2023-04-26T03:11:10.753702Z","shell.execute_reply.started":"2023-04-26T03:11:05.534559Z","shell.execute_reply":"2023-04-26T03:11:10.751281Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"print(torch.cuda.is_available())\nprint(torch.cuda.device_count())\ndevice = torch.device(\"cuda\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T03:11:10.761883Z","iopub.execute_input":"2023-04-26T03:11:10.762678Z","iopub.status.idle":"2023-04-26T03:11:10.821266Z","shell.execute_reply.started":"2023-04-26T03:11:10.762627Z","shell.execute_reply":"2023-04-26T03:11:10.820067Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"True\n1\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/dataset/FinalDataset.csv')\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-26T03:11:10.822972Z","iopub.execute_input":"2023-04-26T03:11:10.823355Z","iopub.status.idle":"2023-04-26T03:11:13.102450Z","shell.execute_reply.started":"2023-04-26T03:11:10.823322Z","shell.execute_reply":"2023-04-26T03:11:13.101316Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                            sentence  class\n0  What city in the United States has the highest...      0\n1            At work, wishing I was out on the boat       0\n2  A smile is a curve that sets everything straig...      0\n3  Does sleep quality mediate the association bet...      0\n4  What city was found on the west bank of the ri...      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentence</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What city in the United States has the highest...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>At work, wishing I was out on the boat</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A smile is a curve that sets everything straig...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Does sleep quality mediate the association bet...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>What city was found on the west bank of the ri...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data['class'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-04-26T03:11:13.107662Z","iopub.execute_input":"2023-04-26T03:11:13.108650Z","iopub.status.idle":"2023-04-26T03:11:13.135676Z","shell.execute_reply.started":"2023-04-26T03:11:13.108607Z","shell.execute_reply":"2023-04-26T03:11:13.131042Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0    400015\n1    376930\nName: class, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\ntrain_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T03:11:13.137719Z","iopub.execute_input":"2023-04-26T03:11:13.138522Z","iopub.status.idle":"2023-04-26T03:11:13.404429Z","shell.execute_reply.started":"2023-04-26T03:11:13.138467Z","shell.execute_reply":"2023-04-26T03:11:13.403003Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# CREATE DATASET CLASS FOR DATALOADERS\nclass Dataset(Dataset):\n    def __init__(self, dataframe):\n        self.texts = dataframe['sentence'].values.tolist()\n        self.labels = dataframe['class'].values.tolist()\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        X = self.texts[idx]\n        y = self.labels[idx]\n        X = X.lower()\n        tokens = nltk.word_tokenize(X, language=\"english\")\n        X = \" \".join(tokens)\n        X = X.strip()\n        \n        return X, y","metadata":{"execution":{"iopub.status.busy":"2023-04-26T03:11:13.406135Z","iopub.execute_input":"2023-04-26T03:11:13.406867Z","iopub.status.idle":"2023-04-26T03:11:13.417337Z","shell.execute_reply.started":"2023-04-26T03:11:13.406820Z","shell.execute_reply":"2023-04-26T03:11:13.416004Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def collate_fn(batch):    \n    x, y = [], []\n    for text, label in batch:\n        x.append(text)\n        y.append(label)\n    padded_text = tokenizer(x, padding=True, truncation=True, return_tensors='pt')\n    \n    return padded_text, torch.FloatTensor(y)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T03:11:13.419311Z","iopub.execute_input":"2023-04-26T03:11:13.419831Z","iopub.status.idle":"2023-04-26T03:11:13.431327Z","shell.execute_reply.started":"2023-04-26T03:11:13.419785Z","shell.execute_reply":"2023-04-26T03:11:13.430001Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class transformer(nn.Module):\n    def __init__(self, base_model):\n        super(transformer, self).__init__()\n\n        self.bert = base_model\n        self.fc1 = nn.Linear(768, 32)\n        self.fc2 = nn.Linear(32, 1)\n\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n        \n    def forward(self, input_ids, attention_mask):\n        bert_out = self.bert(input_ids=input_ids,\n                             attention_mask=attention_mask)[0][:, 0]\n        x = self.fc1(bert_out)\n        x = self.relu(x)\n        \n        x = self.fc2(x)\n        x = self.sigmoid(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-04-26T03:11:13.434390Z","iopub.execute_input":"2023-04-26T03:11:13.435732Z","iopub.status.idle":"2023-04-26T03:11:13.445553Z","shell.execute_reply.started":"2023-04-26T03:11:13.435682Z","shell.execute_reply":"2023-04-26T03:11:13.444201Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(0)\n\nBERT_MODEL = \"roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\nbase_model = AutoModel.from_pretrained(BERT_MODEL)\n\nmodel = transformer(base_model)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T03:11:13.447349Z","iopub.execute_input":"2023-04-26T03:11:13.448609Z","iopub.status.idle":"2023-04-26T03:11:19.298266Z","shell.execute_reply.started":"2023-04-26T03:11:13.448563Z","shell.execute_reply":"2023-04-26T03:11:19.296907Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"# INFERENCE USING STORED MODEL\ndataloader = DataLoader(Dataset(test_data), batch_size=16, num_workers=2, collate_fn=collate_fn)\nmodel = torch.load('/kaggle/input/roberta-model/best_model.pt')\nmodel.eval()\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\ntotal_acc_test = 0\ntotal_loss_test = 0\nfor test_input, test_label in dataloader:\n    attention_mask = test_input['attention_mask'].to(device)\n    input_ids = test_input['input_ids'].squeeze(1).to(device)\n\n    test_label = test_label.to(device)\n\n    output = model(input_ids, attention_mask)\n\n    loss = criterion(output, test_label.float().unsqueeze(1))\n\n    total_loss_test += loss.item()\n\n    pred = (output >= 0.5).int()\n    label = test_label.unsqueeze(1)\n    auc_roc = roc_auc_score(label.cpu(), pred.cpu())\n    total_acc_test += auc_roc\n    \nprint(\"Test Loss:\", total_loss_test/len(dataloader))\nprint(\"Test AUCROC:\", total_acc_test/len(dataloader))","metadata":{"execution":{"iopub.status.busy":"2023-04-26T03:11:19.300633Z","iopub.execute_input":"2023-04-26T03:11:19.301358Z","iopub.status.idle":"2023-04-26T03:19:51.340052Z","shell.execute_reply.started":"2023-04-26T03:11:19.301307Z","shell.execute_reply":"2023-04-26T03:19:51.338693Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Test Loss: 0.10863962155938363\nTest AUCROC: 0.9571639482497114\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}